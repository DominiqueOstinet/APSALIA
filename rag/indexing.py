import os
from pathlib import Path
import hashlib
import shutil
import pandas as pd

from rag.doc_loader import detect_columns, create_smart_chunks_from_detected
from rag.embeddings import get_embedding_model
from rag.elasticsearch_indexer import (
    get_elastic_client,
    create_index_if_not_exists,
    index_documents_bulk,
)

# === üîß CONFIGURATION ===
# indexing.py est plac√© dans /rag (racine du code dans le conteneur)
# et docker-compose monte ./data -> /rag/data
BASE_DIR = Path(__file__).resolve().parent           # /rag
DOCS_DIR = Path(os.getenv("DOCS_DIR", "/data/documents_xlsx"))
INDEX_NAME = os.getenv("ELASTICSEARCH_INDEX", "rfi_rag")

# O√π copier les fichiers sources pour t√©l√©chargement ult√©rieur
SOURCE_STORE_DIR = Path(os.getenv("SOURCE_STORE_DIR", "/data/source_store"))

# Contr√¥le de la suppression de l'index (par d√©faut: False)
REINDEX_DROP = os.getenv("REINDEX_DROP", "false").lower() in {"1", "true", "yes", "y"}


def _sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _enrich_chunks_with_source_metadata(chunks, file_path: Path, stored_path: Path, sha: str) -> None:
    """
    Ajoute les m√©tadonn√©es de tra√ßabilit√© du fichier source sur chaque chunk.
    - source_basename : nom du fichier d'origine
    - source_sha256   : hash du fichier d'origine (anti-duplication/version)
    - source_relpath  : chemin relatif (depuis /) du fichier copi√© dans SOURCE_STORE_DIR
    - content_sha256  : hash du CONTENU du chunk (pour rep√©rer les doublons/√©volutions)
    - obsolete        : drapeau m√©tier (False par d√©faut)
    """
    import hashlib

    rel_from_root = os.path.relpath(str(stored_path), start="/")
    basename = file_path.name
    for doc in chunks:
        if not hasattr(doc, "metadata") or doc.metadata is None:
            doc.metadata = {}

        # hash du contenu du chunk (cha√Æne page_content)
        try:
            content_sha = hashlib.sha256(
                (doc.page_content or "").encode("utf-8", errors="ignore")
            ).hexdigest()
        except Exception:
            content_sha = None

        doc.metadata.update({
            "source_basename": basename,
            "source_sha256": sha,
            "source_relpath": rel_from_root,
            "content_sha256": content_sha,
            "obsolete": False,
        })


def main() -> None:
    # === üöÄ INITIALISATION (faites ici pour √©viter les effets √† l'import) ===
    print("üîå Connexion Elasticsearch‚Ä¶")
    es = get_elastic_client()

    print("üß† Chargement du mod√®le d'embeddings‚Ä¶")
    embedding_model = get_embedding_model()

    # === üßπ GESTION DE L'INDEX ===
    if REINDEX_DROP:
        print(f"üßπ Suppression de l'index existant '{INDEX_NAME}' (REINDEX_DROP=true)‚Ä¶")
        if es.indices.exists(index=INDEX_NAME):
            es.indices.delete(index=INDEX_NAME)
            print(f"   Index '{INDEX_NAME}' supprim√©.")
        else:
            print(f"   Index '{INDEX_NAME}' inexistant, rien √† supprimer.")
    else:
        print("‚ÑπÔ∏è REINDEX_DROP=false ‚Üí on ne supprime pas l'index existant.")

    create_index_if_not_exists(es, INDEX_NAME)
    print(f"‚úÖ Index pr√™t : {INDEX_NAME}")

    # === üìÇ CHARGEMENT & PR√âPARATION DES DOCUMENTS ===
    if not DOCS_DIR.exists():
        raise FileNotFoundError(
            f"Dossier de documents introuvable : {DOCS_DIR}\n"
            "V√©rifie le montage de volume dans docker-compose (./data -> /rag/data)."
        )

    SOURCE_STORE_DIR.mkdir(parents=True, exist_ok=True)

    all_chunks = []
    xlsx_files = list(DOCS_DIR.glob("*.xlsx"))
    if not xlsx_files:
        print(f"‚ö†Ô∏è Aucun fichier .xlsx trouv√© dans {DOCS_DIR}")

    for filepath in xlsx_files:
        print(f"\nüìÑ Fichier : {filepath.name}")

        # 1) Copie du fichier natif (idempotente) + calcul du SHA
        sha = _sha256_file(filepath)
        stored_name = f"{sha}__{filepath.name}"
        stored_path = SOURCE_STORE_DIR / stored_name
        if not stored_path.exists():
            stored_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(str(filepath), str(stored_path))
            print(f"üì• Copie du fichier source ‚Üí {stored_path}")
        else:
            print(f"‚Ü™Ô∏è Copie d√©j√† pr√©sente : {stored_path.name}")

        # 2) D√©tection de la structure (doc_loader) ‚Äî si non d√©tect√©e, on IGNORE le fichier
        try:
            all_sheets = pd.read_excel(filepath, sheet_name=None, header=None)
            onglets_exploitables = detect_columns(all_sheets, filepath.name)
        except Exception as e:
            print(f"‚ùå Erreur lors de la d√©tection de structure pour {filepath.name} : {e}")
            print("‚õî Fichier ignor√© (structure non conforme).")
            continue

        # 3) Chunking m√©tier + enrichissement des m√©tadonn√©es de tra√ßabilit√©
        try:
            for onglet_data in onglets_exploitables:
                chunks = create_smart_chunks_from_detected(onglet_data, filepath.name)
                if not chunks:
                    continue
                _enrich_chunks_with_source_metadata(chunks, filepath, stored_path, sha)
                all_chunks.extend(chunks)
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation des chunks pour {filepath.name} : {e}")
            print("‚õî Fichier ignor√© (√©chec du traitement m√©tier).")
            continue

    print(f"\n‚úÖ Total de chunks d√©tect√©s : {len(all_chunks)}")

    if not all_chunks:
        print("‚ÑπÔ∏è Aucun chunk √† indexer. Fin.")
        return

    # === üß† EMBEDDINGS ===
    print("üîÑ Cr√©ation des embeddings‚Ä¶")
    vectors = embedding_model.embed_documents([doc.page_content for doc in all_chunks])

    # === üì§ INDEXATION ELASTICSEARCH ===
    print("üì¶ Indexation dans Elasticsearch‚Ä¶")
    index_documents_bulk(es, all_chunks, vectors, INDEX_NAME)

    print("üéâ Pipeline termin√© !")


if __name__ == "__main__":
    main()
