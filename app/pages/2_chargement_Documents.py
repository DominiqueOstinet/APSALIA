# app/pages/2_Chargement_de_Documents.py
import os
import tempfile
import pandas as pd
import streamlit as st

from rag.doc_loader import detect_columns, create_smart_chunks_from_detected, KEYWORDS_BESOIN, KEYWORDS_REPONSE
from rag.embeddings import get_embedding_model
from rag.elasticsearch_indexer import get_elastic_client, index_documents_bulk

INDEX_NAME = os.getenv("ELASTICSEARCH_INDEX", "rfi_rag")

st.set_page_config(page_title="Chargement & Indexation", page_icon="üìÅ", layout="wide")
st.header("üìÅ Chargement et Indexation de Nouveaux Documents")

col1, col2 = st.columns([2, 1])

with col1:
    uploaded_files = st.file_uploader(
        "Choisissez vos fichiers Excel (.xlsx, .xls)",
        type=['xlsx', 'xls'],
        accept_multiple_files=True,
        help="S√©lectionnez un ou plusieurs fichiers Excel contenant vos questionnaires clients"
    )

with col2:
    st.subheader("Configuration des mots-cl√©s")

    # BESOIN
    with st.expander("üîç Mots-cl√©s BESOIN", expanded=False):
        st.write("**Par d√©faut :**")
        st.write(", ".join(KEYWORDS_BESOIN))
        new_kw_besoin = st.text_input("Ajouter un mot-cl√© BESOIN:", key="new_besoin")
        if st.button("Ajouter BESOIN", key="add_besoin"):
            if new_kw_besoin and new_kw_besoin.lower() not in [k.lower() for k in KEYWORDS_BESOIN + st.session_state.get("custom_keywords_besoin", [])]:
                st.session_state.custom_keywords_besoin = st.session_state.get("custom_keywords_besoin", []) + [new_kw_besoin.lower()]
                st.success(f"Ajout√©: {new_kw_besoin}")
                st.rerun()
        if st.session_state.get("custom_keywords_besoin"):
            st.write("**Personnalis√©s :**")
            for i, kw in enumerate(st.session_state.custom_keywords_besoin):
                c1, c2 = st.columns([3, 1])
                with c1: st.write(kw)
                with c2:
                    if st.button("‚ùå", key=f"del_besoin_{i}"):
                        st.session_state.custom_keywords_besoin.remove(kw)
                        st.rerun()

    # R√âPONSE
    with st.expander("üí¨ Mots-cl√©s R√âPONSE", expanded=False):
        st.write("**Par d√©faut :**")
        st.write(", ".join(KEYWORDS_REPONSE))
        new_kw_rep = st.text_input("Ajouter un mot-cl√© R√âPONSE:", key="new_reponse")
        if st.button("Ajouter R√âPONSE", key="add_reponse"):
            if new_kw_rep and new_kw_rep.lower() not in [k.lower() for k in KEYWORDS_REPONSE + st.session_state.get("custom_keywords_reponse", [])]:
                st.session_state.custom_keywords_reponse = st.session_state.get("custom_keywords_reponse", []) + [new_kw_rep.lower()]
                st.success(f"Ajout√©: {new_kw_rep}")
                st.rerun()
        if st.session_state.get("custom_keywords_reponse"):
            st.write("**Personnalis√©s :**")
            for i, kw in enumerate(st.session_state.custom_keywords_reponse):
                c1, c2 = st.columns([3, 1])
                with c1: st.write(kw)
                with c2:
                    if st.button("‚ùå", key=f"del_reponse_{i}"):
                        st.session_state.custom_keywords_reponse.remove(kw)
                        st.rerun()

# Traitement
if uploaded_files:
    st.markdown("---")
    st.subheader("Analyse des fichiers")

    if st.button("üì§ Analyser et Indexer", type="primary"):
        try:
            # Combiner mots-cl√©s
            all_kw_besoin = KEYWORDS_BESOIN + st.session_state.get("custom_keywords_besoin", [])
            all_kw_rep = KEYWORDS_REPONSE + st.session_state.get("custom_keywords_reponse", [])

            import rag.doc_loader as doc_loader_module
            original_besoin = doc_loader_module.KEYWORDS_BESOIN
            original_reponse = doc_loader_module.KEYWORDS_REPONSE
            doc_loader_module.KEYWORDS_BESOIN = all_kw_besoin
            doc_loader_module.KEYWORDS_REPONSE = all_kw_rep

            all_documents = []
            progress = st.progress(0)
            status = st.empty()

            for i, uf in enumerate(uploaded_files):
                status.text(f"Traitement: {uf.name}")
                progress.progress(i / max(len(uploaded_files), 1))

                # Sauvegarde temporaire
                with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:
                    tmp.write(uf.getvalue())
                    tmp_path = tmp.name

                try:
                    all_sheets = pd.read_excel(tmp_path, sheet_name=None, header=None)
                    onglets = detect_columns(all_sheets, uf.name)

                    if onglets:
                        st.success(f"‚úÖ {uf.name}: {len(onglets)} onglet(s) exploitable(s)")
                        for onglet_data in onglets:
                            st.write(f"  üìã {onglet_data['onglet']}")
                            st.write(f"    - Besoin d√©tect√©: '{onglet_data['besoin']['contenu']}'")
                            st.write(f"    - {len(onglet_data['reponses'])} colonne(s) de r√©ponse")
                            chunks = create_smart_chunks_from_detected(onglet_data, uf.name)
                            all_documents.extend(chunks)
                    else:
                        st.warning(f"‚ö†Ô∏è {uf.name}: Aucun onglet exploitable trouv√©")
                finally:
                    os.unlink(tmp_path)

            progress.progress(1.0)
            status.text("Analyse termin√©e")

            # Restaurer mots-cl√©s
            doc_loader_module.KEYWORDS_BESOIN = original_besoin
            doc_loader_module.KEYWORDS_REPONSE = original_reponse

            if all_documents:
                st.success(f"üìä Total: {len(all_documents)} chunks cr√©√©s")

                with st.spinner("Cr√©ation des embeddings et indexation..."):
                    model = get_embedding_model()
                    vectors = model.embed_documents([doc.page_content for doc in all_documents])

                    es = get_elastic_client()
                    success = index_documents_bulk(es, all_documents, vectors, INDEX_NAME)

                    if success:
                        st.success("‚úÖ Documents index√©s avec succ√®s!")
                        st.balloons()
                    else:
                        st.error("‚ùå Erreur lors de l'indexation")
            else:
                st.error("‚ùå Aucun document exploitable trouv√© dans les fichiers")

        except Exception as e:
            st.error(f"Erreur lors du traitement: {e}")
            if 'doc_loader_module' in locals():
                doc_loader_module.KEYWORDS_BESOIN = original_besoin
                doc_loader_module.KEYWORDS_REPONSE = original_reponse

# Aper√ßu rapide
if uploaded_files:
    st.markdown("---")
    st.subheader("Aper√ßu des fichiers charg√©s")
    for uf in uploaded_files:
        with st.expander(f"üìÑ {uf.name} ({uf.size} bytes)"):
            try:
                sheets_info = pd.read_excel(uf, sheet_name=None, header=None, nrows=0)
                st.write(f"**Onglets d√©tect√©s:** {list(sheets_info.keys())}")
                first_sheet = list(sheets_info.keys())[0]
                sample = pd.read_excel(uf, sheet_name=first_sheet, nrows=5)
                st.write(f"**Aper√ßu de '{first_sheet}':**")
                st.dataframe(sample)
            except Exception as e:
                st.error(f"Erreur lecture aper√ßu: {e}")
